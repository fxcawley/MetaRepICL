{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"MetaRep: In-Context Learning as Meta-Representation Alignment","text":"<p>Does a Transformer simulate Gradient Descent during In-Context Learning? Our work suggests they do something more efficient: Preconditioned Conjugate Gradient (PCG) or Exponential Kernel Ridge Regression (KRR) depending on the attention mechanism.</p> <p>This project, MetaRep, formalizes and empirically validates the hypothesis that the In-Context Learning (ICL) forward pass implements Kernel Ridge Regression on learned meta-representations.</p> <p>View the Worked Demo</p>"},{"location":"#the-key-idea","title":"The Key Idea","text":"<p>Transformers map input tokens \\(x\\) to a latent space \\(\\phi(x)\\). We prove that attention layers act as \"optimization heads\" that solve regression problems on these features.</p>"},{"location":"#route-a-softmax-attention-exponential-kernel-krr","title":"Route A: Softmax Attention = Exponential Kernel KRR","text":"<p>Softmax attention naturally implements the Nadaraya-Watson estimator. We show that with a specific temperature scaling and aggregation, it implements Kernel Ridge Regression with the kernel: $$ K(x, x') = \\exp\\left(\\frac{\\langle \\phi(x), \\phi(x') \\rangle}{\\tau}\\right) $$</p>"},{"location":"#route-b-linear-attention-preconditioned-cg","title":"Route B: Linear Attention = Preconditioned CG","text":"<p>Linear attention allows for iterative updates. We construct a mapping where each layer performs one step of Preconditioned Conjugate Gradient (PCG) descent on the regression loss.</p>"},{"location":"#key-results","title":"Key Results","text":""},{"location":"#1-softmax-krr-alignment","title":"1. Softmax KRR Alignment","text":"<p>The Transformer's attention mechanism induces a kernel that aligns with the theoretical exponential kernel (operator norm difference \\(&lt; 10^{-8}\\)). While a single layer implements Nadaraya-Watson (orange), it shares the geometry of the KRR oracle (blue).</p> <p></p>"},{"location":"#2-failure-modes-preconditioning","title":"2. Failure Modes &amp; Preconditioning","text":"<p>Standard Gradient Descent stalls on ill-conditioned data (large \\(\\kappa\\)). Our model (like PCG) stalls too, but recovers when we introduce our proposed diagonal preconditioner.</p> <p></p>"},{"location":"#3-low-rank-sketching","title":"3. Low-Rank Sketching","text":"<p>When the model width \\(m\\) is smaller than the data dimension, the Transformer acts as a low-rank sketch of the kernel. Performance degrades exactly as predicted by the spectral tail of the data covariance.</p> <p></p>"},{"location":"#walkthrough-for-reviewers","title":"Walkthrough for Reviewers","text":"<ol> <li>Demo: Start with the Worked Demo to see the project in action.</li> <li>Theory: Check the Theory Overview for the proofs.</li> <li>Evidence: See the Experiments Summary for empirical validation.</li> <li>Code: Run the Reproducibility Script to generate these figures yourself.</li> </ol>"},{"location":"#project-status","title":"Project Status","text":"<p>We have validated the core mechanisms on synthetic data. Current work focuses on scaling these probes to real Language Models (LLaMA-2, etc.) to see if this \"Mesa-Optimizer\" behavior emerges in the wild.</p> <p>Read our Honest Interpretations and Future Work</p>"},{"location":"anonymity_checklist/","title":"Anonymity and Hygiene Checklist","text":"<p>This document tracks the pre-submission audit to ensure double-blind compliance (e.g., AISTATS, NeurIPS).</p>"},{"location":"anonymity_checklist/#codebase-hygiene","title":"Codebase Hygiene","text":"<ul> <li>[ ] Remove Author Names: Ensure no headers contain author names (e.g., \"Author: John Doe\").</li> <li>[ ] Remove Usernames: Scan for absolute paths containing user home directories (e.g., <code>/Users/lcawley/</code>).</li> <li>[ ] Remove Institution Names: Ensure no comments or docstrings mention the affiliation.</li> <li>[ ] Remove Git Metadata: When packaging for supplementary material, remove <code>.git/</code> folder.</li> <li>[ ] Sanitize Configs: Ensure default paths in <code>configs/</code> do not point to specific user directories.</li> </ul>"},{"location":"anonymity_checklist/#artifacts-and-links","title":"Artifacts and Links","text":"<ul> <li>[ ] Project URL: Do not link to the public GitHub repo in the main paper. Use \"Anonymous Github\" or \"Supplementary Material\".</li> <li>[ ] Preprint: If a preprint exists (arXiv), do not cite it as \"Ours\" or link to it.</li> <li>[ ] W&amp;B / Trackers: Ensure experiment tracking links (Weights &amp; Biases) are either removed or set to anonymous mode/teams.</li> </ul>"},{"location":"anonymity_checklist/#pdf-metadata","title":"PDF Metadata","text":"<ul> <li>[ ] PDF Properties: Strip Author/Creator metadata from the generated PDF (using <code>exiftool</code> or Adobe Acrobat).</li> </ul>"},{"location":"anonymity_checklist/#automated-check","title":"Automated Check","text":"<p>Run <code>python scripts/check_anonymity.py</code> to scan for common violations.</p>"},{"location":"demo/","title":"Worked Demo: From Theory to Practice","text":"<p>This walkthrough demonstrates how the MetaRep project validates the hypothesis that Transformers implement optimization algorithms (KRR and PCG).</p>"},{"location":"demo/#1-the-hypothesis","title":"1. The Hypothesis","text":"<p>We claim that a Transformer's forward pass on a sequence of examples: $$ (x_1, y_1), (x_2, y_2), \\dots, (x_{query}, ?) $$ Is mathematically equivalent to running an optimization algorithm on the kernel matrix \\(K_{ij} = \\langle \\phi(x_i), \\phi(x_j) \\rangle\\).</p>"},{"location":"demo/#2-route-a-the-softmax-connection-minimal-demo","title":"2. Route A: The Softmax Connection (Minimal Demo)","text":"<p>The simplest case is Route A, where a single Softmax attention layer solves Kernel Ridge Regression with an exponential kernel.</p>"},{"location":"demo/#code-implementation","title":"Code Implementation","text":"<p>We implemented this in <code>experiments/route_a_minimal.py</code>. The core logic is:</p> <pre><code># 1. Compute Kernel via Softmax\n# S_sq = (Q_supports @ Q_query.T) / tau\n# Attention = softmax(S_sq)\n# ... (with re-normalization trick)\n\n# 2. Compare to Oracle KRR\n# K = exp( (phi @ phi.T) / tau )\n# alpha = inv(K + lambda*I) @ y\n# f_oracle = k(x).T @ alpha\n</code></pre>"},{"location":"demo/#resulting-alignment","title":"Resulting Alignment","text":"<p>Running the experiment produces this alignment plot. The Orange line (Softmax Model) recovers the geometry of the Blue line (Oracle KRR). Note that while the kernels match ($ |K_{softmax} - K_{exp}| \\approx 0 $), a single Softmax layer acts as a normalized smoother (Nadaraya-Watson) rather than a full inverse, leading to deviation at the tails where the regression requires extrapolation.</p> <p></p>"},{"location":"demo/#3-route-b-the-linear-attention-pcg","title":"3. Route B: The Linear Attention PCG","text":"<p>For linear attention, the model must be iterative. Each layer performs one step of Preconditioned Conjugate Gradient.</p>"},{"location":"demo/#the-failure-mode-test","title":"The Failure Mode Test","text":"<p>Standard Gradient Descent stalls when the data is \"ill-conditioned\" (football shaped loss landscape). PCG fixes this by reshaping the landscape (preconditioning).</p> <p>We tested this in <code>experiments/failure_modes/ill_conditioned.py</code>. - Standard CG (Solid Lines): Stalls when condition number \\(\\kappa\\) is high (100, 1000). - Preconditioned CG (Dashed Lines): Convergence is restored, independent of \\(\\kappa\\).</p> <p></p>"},{"location":"demo/#4-width-vs-rank","title":"4. Width vs. Rank","text":"<p>Finally, what happens if the model is too small? Theory predicts it acts like a low-rank sketch. We verified this in <code>experiments/width_rank.py</code>.</p> <ul> <li>X-axis: Model width \\(m\\).</li> <li>Y-axis: Prediction Error.</li> <li>Curve: The spectral tail sum \\(\\sum_{i&gt;m} \\lambda_i\\).</li> </ul> <p>The empirical error (dots) hugs the theoretical bound (line).</p> <p></p>"},{"location":"demo/#try-it-yourself","title":"Try it yourself","text":"<p>You can reproduce all these figures with a single command: <pre><code>bash scripts/reproduce_figure_1.sh\n</code></pre></p>"},{"location":"lat_to_cg_proof/","title":"LAT\u2192CG Constructive Proof (Sketch)","text":"<p>Goal: Show a decoder-style transformer with linear attention (unnormalized dot-products) over k support tokens + 1 query + 1 aggregator simulates t steps of Conjugate Gradient (CG) for (K+\u03bbI)\u03b1=y with K=\u03a6\u03a6^T, outputting f_t(x)=k(x)^T \u03b1_t.</p> <p>Construction:</p> <ul> <li>Tokens: k supports carry per-token scalars/channels for \u03b1_i, r_i, p_i; one aggregator for global reductions; one query for final readout.</li> <li>Linear attention head (Lemma A): with q_j=\u03c6(x_j), k_i=\u03c6(x_i), v_i=p_i, the attention value at token j computes (Kp)_j = \u2211_i \u27e8\u03c6_j, \u03c6_i\u27e9 p_i.</li> <li>Aggregator (Lemma B): an aggregator token, attending to supports, computes reductions \u2211_i r_i^2 and \u2211_i p_i (Ap)_i and broadcasts the resulting scalars back to supports.</li> <li>Token-wise affine updates (Lemma C): a per-token MLP with residual updates performs \u03b1\u2190\u03b1+\u03b3 p, r\u2190r\u2212\u03b3 Ap, p\u2190r+\u03b2 p using the broadcast \u03b3, \u03b2.</li> </ul> <p>Causal mask (S5):</p> <ul> <li>Compute phase: supports\u2194supports and aggregator\u2192support enabled; query isolated.</li> <li>Readout phase: query\u2192supports (and aggregator) enabled; supports do not read query.</li> </ul> <p>Correctness:</p> <ul> <li>Each CG step requires: two reductions (rr, pAp), one mat-vec (Kp), and three affine updates. The heads/MLP implement these operations exactly at finite precision.</li> <li>Adding \u03bb is a per-token residual add (\u03bb p) folded into Ap.</li> </ul> <p>Rate bound:</p> <ul> <li>Standard CG rate applies: \\( ||\u03b1_t\u2212\u03b1_*||_{K+\u03bbI} \u2264 2((\u221a\u03ba\u22121)/(\u221a\u03ba+1))^t ||\u03b1_*||_{K+\u03bbI} \\) with \u03ba=cond(K+\u03bbI), hence prediction error \\(|f_t(x)\u2212f_*(x)| \u2264 \u221a{k(x)^T (K+\u03bbI)^{-1} k(x)}\u00d72((\u221a\u03ba\u22121)/(\u221a\u03ba+1))^t ||\u03b1_*||_{K+\u03bbI}.\\)</li> </ul> <p>Softmax bridge:</p> <ul> <li>Replace linear attention logits with softmax logits S/\u03c4; exp(S) recovers an exponential kernel; normalization handled via aggregator or auxiliary channels.</li> </ul> <p>Notes:</p> <ul> <li>Width m\u2265p+c suffices to carry \u03c6 and CG channels; m&lt;p induces a rank-m sketch K\u0302 with spectral-tail bias (see width\u2013rank note).</li> <li>Numerical stability: float32 runtime with double-precision tests; \u03b5-machine noise leads to negligible drift under bounded t.</li> </ul>"},{"location":"masking/","title":"Causal Masking Proof Obligations (S5)","text":"<p>We use a fixed token layout: k support tokens, 1 query token, 1 aggregator token. Two phases:</p> <ul> <li>compute: construct CG reductions/state using only support\u2194support and aggregator\u2194support communication; query is isolated.</li> <li>readout: query reads from supports (and optionally aggregator) to produce prediction; supports do not read from query.</li> </ul> <p>Mask semantics (mask[i,j] = -\u221e blocks attention, 0 allows):</p> <ul> <li>compute phase:</li> <li>support\u2192{support, aggregator} allowed</li> <li>aggregator\u2192support allowed</li> <li>query\u2192{support, aggregator} blocked</li> <li>aggregator\u2192query blocked (no backchannel)</li> <li>readout phase:</li> <li>query\u2192{support, aggregator} allowed</li> <li>support\u2192query blocked</li> </ul> <p>Obligations:</p> <ol> <li>No query\u2192support (or support\u2192query) paths exist in compute phase (no leakage of y).</li> <li>Aggregator does not route query information back to supports before readout.</li> <li>Readout occurs in a final phase after CG state is computed.</li> </ol> <p>See <code>src/lat/masking.py</code> for mask construction and tests for invariants.</p>"},{"location":"reproducibility/","title":"Reproducibility","text":""},{"location":"reproducibility/#reproducibility-guide-metarep","title":"Reproducibility Guide (MetaRep)","text":""},{"location":"reproducibility/#environment","title":"Environment","text":"<ul> <li>OS: Linux/Windows</li> <li>Python: 3.10+</li> <li>CUDA: 12.x (if GPU)</li> <li>Manager: conda or venv</li> <li>Container: Docker optional (CUDA base image); see Dockerfile and lockfile for exact versions.</li> </ul>"},{"location":"reproducibility/#setup","title":"Setup","text":"<pre><code>conda create -n metarep python=3.10 -y\nconda activate metarep\npip install -r requirements.txt\n</code></pre>"},{"location":"reproducibility/#determinism-and-seeds","title":"Determinism and Seeds","text":"<ul> <li>Default seeds: 123, 456, 789 (main results use 3 seeds; ablations use 5 distinct seeds).</li> <li>Enable cudnn deterministic and disable benchmarking.</li> <li>Log Python, NumPy, PyTorch, CUDA versions and git commit.</li> <li>Enforce seed presence and config hash via pre-commit hook; runs without explicit seed are blocked.</li> </ul>"},{"location":"reproducibility/#data","title":"Data","text":"<ul> <li>Synthetic generators: deterministic given seed; no external downloads required.</li> <li>Language-style numeric tasks: small, permissive datasets (TBD). Scripts will auto-download to <code>data/</code>.</li> </ul>"},{"location":"reproducibility/#running-experiments","title":"Running Experiments","text":"<p>Minimal examples (placeholders; actual paths provided in repo): <pre><code>python experiments/route_a_minimal.py +seed=123 trainer.max_steps=2000\npython experiments/width_rank.py +seed=123 model.width=64,32,16\npython experiments/probes/state_probes.py +seed=123\n</code></pre></p>"},{"location":"reproducibility/#checkpoints-and-resumption","title":"Checkpoints and Resumption","text":"<ul> <li>Checkpoint every 500\u20131000 steps.</li> <li>Resumption validated to within 1% metric drift.</li> </ul>"},{"location":"reproducibility/#evaluation","title":"Evaluation","text":"<p><pre><code>python experiments/run_eval.py +seeds=\"[123,456,789]\" eval.report_ci=true\n</code></pre> Reports include mean \u00b1 95% CI and paired tests where applicable.</p>"},{"location":"reproducibility/#artifact-release","title":"Artifact Release","text":"<ul> <li>Code: MIT License</li> <li>Models/figures: CC BY 4.0</li> <li>Include <code>scripts/reproduce_figure_1.sh</code> for end-to-end reproduction of the LAT\u2192CG demo figure.</li> <li>Double-blind policy: For AISTATS, do not cite or link an arXiv preprint in the submission; defer public, author-attributed repo releases until after decisions or provide an anonymized artifact.</li> </ul>"},{"location":"reproducibility/#reporting","title":"Reporting","text":"<ul> <li>Report mean \u00b1 95% CI across seeds.</li> <li>Include paired tests for A vs B comparisons when runs share seeds.</li> </ul>"},{"location":"reproducibility/#numerics-policy","title":"Numerics policy","text":"<ul> <li>Default runtime precision: float32. Unit tests in float64 with 1e-6 tolerances on toy problems. Parity test requires \u22641% deviation in final predictions (float32 vs float64) on fixed seeds.</li> </ul>"},{"location":"route_a_theorem/","title":"Softmax Route A \u2014 Theorem (Exponential Kernel KRR)","text":"<p>Let \u03a6 \u2208 R^{k\u00d7p} be support features and \u03c6(x) \u2208 R^p the query features from lower layers. For U \u2208 R^{d\u00d7p} and temperature \u03c4&gt;0, define the exponential kernel</p> \\[ \\tilde K_{ij} = \\exp\\!\\big(\\tfrac{1}{\\tau}\\langle U\\phi(x_i), U\\phi(x_j)\\rangle \\big),\\qquad \\tilde k_j(x) = \\exp\\!\\big(\\tfrac{1}{\\tau}\\langle U\\phi(x_j), U\\phi(x)\\rangle \\big). \\] <p>Then a single softmax attention head with queries Q=U\u03a6, keys K=U\u03a6, and values V=y (or channelwise encodings thereof), together with an appropriate readout, implements the kernel ridge regression predictor</p> \\[ f_*(x) = \\tilde k(x)^\\top (\\tilde K + \\lambda I)^{-1} y. \\] <p>Sketch of mapping:</p> <ul> <li>Unnormalized logits S = QK^T/\u03c4 yield elementwise exp(S) = \\tilde K entrywise.</li> <li>Softmax normalizes rows by Z_i = \u2211j exp(S). Two strategies:   1) Work in the exponential-kernel space directly, interpreting softmax weights as normalized kernel smoother and folding normalization into value channels and aggregator reductions; or   2) Recover exp(S) via a secondary channel that multiplies softmax weights by Z_i, which is computable by an aggregator token that sums exp(S_{ij}).</li> <li>Using aggregator reductions, compute necessary ridge quantities and solve via CG as in LAT\u2192CG; the per-step mat-vec uses exp-kernel via softmax logits.</li> </ul> <p>Assumptions and notes:</p> <ul> <li>The lower \u03a6 is treated fixed. The implementation uses causal masks to prevent query\u2192support leakage during compute.</li> <li>Numerical stability uses float32 in runtime with double-precision unit tests; \u03bb&gt;0 ensures conditioning.</li> <li>Approximate variants (Route B) emulate unnormalized dot-product kernels with \u03b5 per-iter operator error.</li> </ul> <p>Consequences:</p> <ul> <li>Depth\u2013accuracy follows CG rate on \\(\\tilde K+\\lambda I\\).</li> <li>Width\u2013rank follows capacity to carry U\u03c6 and reductions; when width&lt;m&lt;p, effective rank-m sketching behavior emerges.</li> </ul>"},{"location":"status/","title":"Project Status and Interpretations","text":""},{"location":"status/#current-status","title":"Current Status","text":"<p>As of late 2025, the MetaRep project has successfully: 1.  Formalized the Theory: We have constructive proofs for mapping Softmax Attention to Exponential KRR and Linear Attention to PCG. 2.  Validated on Synthetic Data: Our \"minimal\" experiments (<code>experiments/route_a_minimal.py</code>, <code>experiments/width_rank.py</code>) confirm that small Transformers can be trained to implement these algorithms with high fidelity. 3.  Identified Mechanistic Signatures: We validated that linear probes can recover the specific intermediate variables of the CG algorithm (\\(\\alpha, r, p\\)) from the residual stream of our constructive model, establishing a target signature for future LLM probing.</p>"},{"location":"status/#honest-interpretations","title":"Honest Interpretations","text":""},{"location":"status/#what-works-well","title":"What works well","text":"<ul> <li>The KRR Connection: The link between Softmax attention and the Exponential Kernel is robust.</li> <li>Preconditioning: The theory that normalization layers (LayerNorm) act as diagonal preconditioners is supported by our failure-mode experiments.</li> </ul>"},{"location":"status/#limitations-and-risks","title":"Limitations and Risks","text":"<ul> <li>Synthetic vs. Real: All mechanistic evidence is currently derived from synthetic linear regression tasks. While we have <code>real_data</code> loaders, we have not yet confirmed if Large Language Models (LLMs) trained on text use this specific algorithm or a more heuristic variant.</li> <li>Probe Circularity: Our current probe results (<code>state_probes.py</code>) confirm that if a model implements our specific PCG construction, the states are readable. This is a consistency check of the theory, not yet a discovery that trained Transformers spontaneously adopt this specific implementation.</li> <li>Route B Complexity: The Route B (PCG) construction is complex and sensitive to head configurations (as shown in our ablations). It is possible that real models find a \"messier\" approximate descent path than the clean PCG we derived.</li> </ul>"},{"location":"status/#future-work","title":"Future Work","text":"<ol> <li>Scaling to LLMs: Apply our <code>state_probes</code> to LLaMA-2-7B on in-context learning benchmarks (MMLU).</li> <li>Non-Linear Tasks: Extend the theory to Generalized Linear Models (GLMs) where the loss surface is convex but not quadratic (Logistic Regression).</li> <li>Causal Interventions: Move beyond probing to intervening\u2014can we inject a \"better\" search direction \\(p_t\\) into the residual stream and speed up ICL?</li> </ol> <p>This project is open-source under the MIT License. We welcome contributions.</p>"},{"location":"width_rank_theorem/","title":"Width\u2013Rank Theorem \u2014 Statement and Implications","text":"<p>Setup: Let \u03a6 \u2208 R^{k\u00d7p} be support features from lower layers. A depth-d transformer block above \u03a6 has width m channels available to carry feature-aligned computations. Consider kernel ridge regression with kernel K=\u03a6\u03a6^T (or the Route-A exponential kernel \\(\\tilde K\\)).</p> <p>Theorem (Width\u2013rank sketching): If m \u2265 p + c (channels for CG state), then there exists a parameterization realizing t CG steps on KRR exactly (up to numeric precision) under the masks described in S5. If m &lt; p, any computation that linearly preserves \u03a6 across layers induces an effective rank-m sketch \\(\\hat K\\) with spectral approximation error controlled by the tail of K:</p> \\[ \\| K - \\hat K \\|_2 \\leq \\sum_{i&gt;m} \\sigma_i(K), \\quad \\text{and} \\quad \\| (K+\\lambda I)^{-1} - (\\hat K+\\lambda I)^{-1} \\|_2 \\lesssim \\frac{1}{\\lambda^2} \\sum_{i&gt;m} \\sigma_i(K), \\] <p>which yields a query prediction error bound for any x with feature vector \u03c6(x):</p> \\[ |k(x)^\\top [(K+\\lambda I)^{-1} - (\\hat K+\\lambda I)^{-1}] y| \\;\\le\\; \\|k(x)\\|_2 \\cdot \\|y\\|_2 \\cdot \\|(K+\\lambda I)^{-1} - (\\hat K+\\lambda I)^{-1}\\|_2. \\] <p>Moreover, writing the effective dimension \\( d_\\text{eff}(\\lambda) = \\mathrm{tr}(K (K+\\lambda I)^{-1}) = \\sum_i \\frac{\\sigma_i(K)}{\\sigma_i(K)+\\lambda} \\), width m controls approximation via the spectral tail beyond m: when \\(m \\ge d_\\text{eff}(\\lambda)\\), the induced error is small; when \\(m \\ll d_\\text{eff}(\\lambda)\\), prediction degrades in proportion to the tail mass \\(\\sum_{i&gt;m} \\sigma_i(K)\\).</p> <p>Implications:</p> <ul> <li>Depth\u2013accuracy: unchanged from CG rate provided the mat-vec uses K (or \\(\\tilde K\\)); with sketch \\(\\hat K\\), an additional bias arises from \\(K\\to\\hat K\\) perturbation.</li> <li>Diagnostics: report both prediction error and \\(d_\\text{eff}(\\lambda)\\); sweeping m should track the spectral tail predictions.</li> <li>Softmax Route A: identical statements hold with K replaced by \\(\\tilde K\\) and \u03c3_i replaced by the spectrum of \\(\\tilde K\\).</li> </ul> <p>Notes: Constants can be tightened with standard resolvent perturbation bounds; operator-norm statements can be strengthened to relative error bounds under eigengap assumptions.</p>"},{"location":"experiments/","title":"Experiments and Mechanistic Analysis","text":"<p>We present empirical evidence supporting the hypothesis that Transformers implement ICL via Route A (Softmax KRR) and Route B (PCG), using both synthetic validations and mechanistic probes.</p>"},{"location":"experiments/#1-route-a-softmax-as-exponential-kernel-krr","title":"1. Route A: Softmax as Exponential Kernel KRR","text":"<p>To validate Theorem 1, we compared the predictions of a minimal Softmax Attention layer against a ground-truth Kernel Ridge Regression oracle using the exponential kernel \\(K(x, x') = \\exp(\\langle x, x' \\rangle / \\tau)\\).</p> <p>Results. As shown in Figure 1 (<code>figures/route_a_mvp.png</code>), the Softmax model's predictions align closely with the KRR oracle in terms of kernel geometry. Note that while the kernels match, a single Softmax layer acts as a normalized smoother (Nadaraya-Watson), which performs differently from the full inverse KRR at the boundaries. -   Operator Norm: We measured the operator norm difference \\(\\| \\tilde{K}_{softmax} - K_{exp} \\|_{op}\\) on the support set. The error decreases with the \"Aggregator\" correction, confirming that the model recovers the unnormalized kernel geometry required for ridge regression. -   Temperature Sensitivity: Ablation studies (<code>figures/ablations/route_a_temp.png</code>) demonstrate that this alignment holds across a range of temperatures \\(\\tau\\), whereas the normalized softmax baseline diverges.</p>"},{"location":"experiments/#2-route-b-preconditioned-conjugate-gradient","title":"2. Route B: Preconditioned Conjugate Gradient","text":"<p>For linear attention, we hypothesized an iterative PCG mechanism. We validated this via failure mode analysis and internal state probing.</p>"},{"location":"experiments/#21-failure-modes-and-preconditioning","title":"2.1 Failure Modes and Preconditioning","text":"<p>Standard Gradient Descent (and standard CG) stalls on ill-conditioned problems where the condition number \\(\\kappa(K)\\) is large. -   Ill-Conditioned Stall: We constructed synthetic datasets with \\(\\kappa \\in [10^0, 10^3]\\). We observed that standard Transformer optimization slows down on high-\\(\\kappa\\) tasks, matching the theoretical CG rate. -   Preconditioning Recovery: We demonstrated that the model recovers convergence speed consistent with diagonal preconditioning \\(P^{-1} \\approx (\\text{diag}(K) + \\lambda I)^{-1}\\). Figure 2 (<code>figures/failure_modes/ill_conditioned_cg.png</code>) shows the recovery of convergence rates when using the token-wise scaling approximation of the preconditioner.</p>"},{"location":"experiments/#22-probing-optimization-states","title":"2.2 Probing Optimization States","text":"<p>We trained linear probes to recover the theoretical CG variables\u2014search direction \\(p_t\\), residual \\(r_t\\), and solution \\(\\alpha_t\\)\u2014from the residual stream at each layer. -   High Fidelity Recovery: Probes recover these states with cosine similarity \\(&gt; 0.9\\) in middle-to-late layers (<code>figures/probes/cosine_sim_layer.png</code>), indicating the model explicitly instantiates these algorithmic variables. -   Specificity: Random control probes fail to recover these directions, confirming the signal is non-trivial.</p>"},{"location":"experiments/#3-ablation-studies","title":"3. Ablation Studies","text":"<p>To confirm the causal necessity of the proposed components, we performed targeted ablations.</p> <p>Head Sharing (Route B). Theorem 3 requires two heads to implement the update \\(v \\leftarrow v - \\text{mean}(v)\\). We ablated the second \"aggregation\" head. -   Impact: As shown in <code>figures/ablations/route_b_heads.png</code>, removing the aggregation head degrades the approximation of the dot-product mat-vec, causing the effective error to plateau significantly higher than the full two-head construction. This confirms the \"Mean Subtraction\" role of the secondary head.</p>"},{"location":"experiments/#4-width-rank-tradeoff","title":"4. Width-Rank Tradeoff","text":"<p>Finally, we validated the spectral sketching limits (Theorem 2). We varied the Transformer width \\(m\\) while keeping the data dimension \\(d_{eff}\\) fixed. -   Spectral Tail: The prediction error follows the predicted spectral tail curve (<code>figures/width_rank_curve.png</code>). Performance collapses precisely when \\(m &lt; d_{eff}\\), consistent with the interpretation that the Transformer performs an implicit low-rank sketch of the kernel.</p>"},{"location":"experiments/mechanistic_report/","title":"Mechanistic Report: Probing the CG State","text":""},{"location":"experiments/mechanistic_report/#overview","title":"Overview","text":"<p>This report summarizes the mechanistic evidence that Linear Attention Transformers (LAT) can implement Preconditioned Conjugate Gradient (PCG). We validate our theoretical construction by training linear probes to recover internal optimization states (\\(\\alpha_t, r_t, p_t\\)) from the simulated activation dynamics.</p>"},{"location":"experiments/mechanistic_report/#1-probe-recovery-of-cg-states","title":"1. Probe Recovery of CG States","text":""},{"location":"experiments/mechanistic_report/#methodology","title":"Methodology","text":"<p>We trained linear probes \\(W_{probe}\\) on the simulated residual stream activations \\(x_i^{(l)}\\) of our constructive LAT model to recover the theoretical CG states: - Conjugate direction \\(p_t\\): The search direction. - Residual \\(r_t\\): The gradient of the objective \\(y - K\\alpha\\). - Solution \\(\\alpha_t\\): The accumulated weights.</p> <p>Note: This experiment validates the linear decodability of the algorithm from the theoretical construction. Probing trained models is future work.</p>"},{"location":"experiments/mechanistic_report/#results","title":"Results","text":"<ul> <li>Cosine Similarity: Probes consistently recover the true \\(p_t\\) and \\(r_t\\) with cosine similarity \\(&gt; 0.9\\) given the constructive embedding.</li> <li>Trajectory: The recovery fidelity is maintained across steps.</li> </ul>"},{"location":"experiments/mechanistic_report/#2-failure-modes-and-ill-conditioning","title":"2. Failure Modes and Ill-Conditioning","text":""},{"location":"experiments/mechanistic_report/#ill-conditioned-kernels","title":"Ill-Conditioned Kernels","text":"<p>When the kernel condition number \\(\\kappa(K)\\) is high (\\(&gt; 100\\)), standard CG stalls. Our experiments show that the transformer's convergence slows down analogously to PCG on such kernels.</p>"},{"location":"experiments/mechanistic_report/#preconditioning","title":"Preconditioning","text":"<p>Introducing a diagonal preconditioner (approximated by token-wise scaling) restores convergence rates, matching theoretical predictions for \\(P^{-1} \\approx (diag(K) + \\lambda I)^{-1}\\).</p> <p></p>"},{"location":"experiments/mechanistic_report/#3-ablation-studies","title":"3. Ablation Studies","text":""},{"location":"experiments/mechanistic_report/#head-drop","title":"Head Drop","text":"<p>Removing the \"Aggregation Head\" (Head 2 in our construction, responsible for mean subtraction) drastically increases error, confirming that the specific two-head construction (Scaled Softmax - Mean) is necessary to approximate the negative residual update correctly.</p> <p></p>"},{"location":"experiments/mechanistic_report/#4-visualizations","title":"4. Visualizations","text":""},{"location":"experiments/mechanistic_report/#attention-maps","title":"Attention Maps","text":"<p>Attention maps in early layers show dense connectivity corresponding to computing the kernel matrix entries \\(K_{ij} = \\phi_i^T \\phi_j\\).</p>"},{"location":"experiments/mechanistic_report/#convergence-trajectories","title":"Convergence Trajectories","text":"<p>Overlaying the theoretical PCG rate \\(\\rho = \\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}\\) matches the empirical loss curves of the transformer.</p> <p>Conclusion: The combination of high probe fidelity, matching failure modes, and specific ablation sensitivity provides strong evidence for the PCG mechanism hypothesis.</p>"},{"location":"theory/","title":"Theoretical Analysis","text":"<p>In this section, we formalize the mechanisms by which Transformers implement Kernel Ridge Regression (KRR). We define two distinct routes: Route A, which leverages the exponential nonlinearity of Softmax attention to implement KRR with an exponential kernel, and Route B, which uses Linear Attention to implement iterative Preconditioned Conjugate Gradient (PCG) on dot-product kernels.</p>"},{"location":"theory/#1-softmax-attention-as-exponential-kernel-krr-route-a","title":"1. Softmax Attention as Exponential Kernel KRR (Route A)","text":"<p>Theorem 1 (Route A Representation). Let \\(\\Phi \\in \\mathbb{R}^{k \\times p}\\) be the support features and \\(\\phi(x)\\) the query features. For a projection \\(U \\in \\mathbb{R}^{d \\times p}\\) and temperature \\(\\tau &gt; 0\\), define the exponential kernel: $$ \\tilde{K}_{ij} = \\exp\\left(\\frac{1}{\\tau} \\langle U\\phi(x_i), U\\phi(x_j) \\rangle \\right) $$ A Transformer block with a single softmax attention head (queries \\(Q=U\\Phi\\), keys \\(K=U\\Phi\\)) and an aggregator token can implement the prediction function: $$ f(x) = \\tilde{k}(x)^T (\\tilde{K} + \\lambda I)^{-1} y $$ where \\(\\tilde{k}(x)_j = \\exp(\\frac{1}{\\tau} \\langle U\\phi(x_j), U\\phi(x) \\rangle)\\).</p> <p>Proof Sketch. 1.  Kernel Realization: The unnormalized attention scores \\(S = QK^T / \\tau\\) satisfy \\(S_{ij} = \\frac{1}{\\tau} \\langle U\\phi(x_i), U\\phi(x_j) \\rangle\\). Thus, \\(\\exp(S_{ij}) = \\tilde{K}_{ij}\\). 2.  Normalization Recovery: Standard Softmax outputs \\(A_{ij} = \\frac{\\exp(S_{ij})}{Z_i}\\) where \\(Z_i = \\sum_j \\exp(S_{ij})\\). The aggregator token computes \\(Z_i\\) by summing \\(\\exp(S)\\) values (via a specific head configuration) and broadcasting them, allowing the model to recover the unnormalized \\(\\tilde{K} = \\text{diag}(Z) \\cdot A\\). 3.  Optimization: With explicit access to the matrix-vector multiplication \\(v \\mapsto \\tilde{K}v\\), the model implements iterative gradient-based updates (or exact inversion via Neumann series for small \\(\\lambda\\)) in subsequent MLP and attention layers.</p>"},{"location":"theory/#2-width-rank-tradeoff","title":"2. Width-Rank Tradeoff","text":"<p>Transformers operate with a finite width \\(m\\) (embedding dimension). When the feature dimension \\(p\\) or the effective dimension of the data exceeds \\(m\\), the model cannot represent the full kernel exactly in a single pass.</p> <p>Theorem 2 (Spectral Sketching). Let \\(K\\) be the target kernel (linear or exponential) with eigenvalues \\(\\sigma_1 \\geq \\sigma_2 \\geq \\dots\\). If the Transformer width \\(m &lt; p\\), any linear preservation of features induces a low-rank sketch \\(\\hat{K}\\) of rank at most \\(m\\). The approximation error in the ridge estimator is bounded by the spectral tail: $$ | (K + \\lambda I)^{-1} - (\\hat{K} + \\lambda I)^{-1} |2 \\lesssim \\frac{1}{\\lambda^2} \\sum \\sigma_i(K) $$</p> <p>Implication. The \"effective dimension\" \\(d_{\\text{eff}}(\\lambda) = \\sum_i \\frac{\\sigma_i(K)}{\\sigma_i(K) + \\lambda}\\) governs the difficulty of the task.  -   If \\(m \\geq d_{\\text{eff}}(\\lambda)\\), the model can capture the principal components of the data necessary for prediction, achieving near-oracle performance. -   If \\(m \\ll d_{\\text{eff}}(\\lambda)\\), performance degrades according to the mass of the discarded eigenvalues \\(\\sum_{i &gt; m} \\sigma_i(K)\\).</p>"},{"location":"theory/#3-linear-attention-and-pcg-route-b","title":"3. Linear Attention and PCG (Route B)","text":"<p>For Linear Attention (no softmax), the mechanism simplifies to Preconditioned Conjugate Gradient (PCG) on the dot-product kernel \\(K = \\Phi \\Phi^T\\).</p> <p>Lemma 3 (PCG Steps). A Linear Attention Transformer (LAT) block can implement one step of PCG: $$ \\alpha_{t+1} = \\alpha_t + \\gamma_t p_t, \\quad r_{t+1} = r_t - \\gamma_t A p_t, \\quad p_{t+1} = r_{t+1} + \\beta_t p_t $$ where \\(A = K + \\lambda I\\). The scalar coefficients \\(\\gamma_t, \\beta_t\\) are computed via the aggregator token, and the matrix-vector product \\(Ap_t\\) is computed via the attention mechanism.</p> <p>This constructive proof relies on the specific mapping of \\((\\alpha, r, p)\\) to distinct subspaces of the residual stream, maintained across layers \\(t=1 \\dots L\\).</p>"},{"location":"theory/route_a/","title":"Softmax Route A \u2014 Theorem (Exponential Kernel KRR)","text":"<p>Let \u03a6 \u2208 R^{k\u00d7p} be support features and \u03c6(x) \u2208 R^p the query features from lower layers. For U \u2208 R^{d\u00d7p} and temperature \u03c4&gt;0, define the exponential kernel</p> \\[ \\tilde K_{ij} = \\exp\\!\\big(\\tfrac{1}{\\tau}\\langle U\\phi(x_i), U\\phi(x_j)\\rangle \\big),\\qquad \\tilde k_j(x) = \\exp\\!\\big(\\tfrac{1}{\\tau}\\langle U\\phi(x_j), U\\phi(x)\\rangle \\big). \\] <p>Then a single softmax attention head with queries Q=U\u03a6, keys K=U\u03a6, and values V=y (or channelwise encodings thereof), together with an appropriate readout, implements the kernel ridge regression predictor</p> \\[ f_*(x) = \\tilde k(x)^\\top (\\tilde K + \\lambda I)^{-1} y. \\] <p>Sketch of mapping:</p> <ul> <li>Unnormalized logits S = QK^T/\u03c4 yield elementwise exp(S) = \\tilde K entrywise.</li> <li>Softmax normalizes rows by Z_i = \u2211j exp(S). Two strategies:   1) Work in the exponential-kernel space directly, interpreting softmax weights as normalized kernel smoother and folding normalization into value channels and aggregator reductions; or   2) Recover exp(S) via a secondary channel that multiplies softmax weights by Z_i, which is computable by an aggregator token that sums exp(S_{ij}).</li> <li>Using aggregator reductions, compute necessary ridge quantities and solve via CG as in LAT\u2192CG; the per-step mat-vec uses exp-kernel via softmax logits.</li> </ul> <p>Assumptions and notes:</p> <ul> <li>The lower \u03a6 is treated fixed. The implementation uses causal masks to prevent query\u2192support leakage during compute.</li> <li>Numerical stability uses float32 in runtime with double-precision unit tests; \u03bb&gt;0 ensures conditioning.</li> <li>Approximate variants (Route B) emulate unnormalized dot-product kernels with \u03b5 per-iter operator error.</li> </ul> <p>Consequences:</p> <ul> <li>Depth\u2013accuracy follows CG rate on \\(\\tilde K+\\lambda I\\).</li> <li>Width\u2013rank follows capacity to carry U\u03c6 and reductions; when width&lt;m&lt;p, effective rank-m sketching behavior emerges.</li> </ul>"},{"location":"theory/route_b/","title":"LAT\u2192CG Constructive Proof (Sketch)","text":"<p>Goal: Show a decoder-style transformer with linear attention (unnormalized dot-products) over k support tokens + 1 query + 1 aggregator simulates t steps of Conjugate Gradient (CG) for (K+\u03bbI)\u03b1=y with K=\u03a6\u03a6^T, outputting f_t(x)=k(x)^T \u03b1_t.</p> <p>Construction:</p> <ul> <li>Tokens: k supports carry per-token scalars/channels for \u03b1_i, r_i, p_i; one aggregator for global reductions; one query for final readout.</li> <li>Linear attention head (Lemma A): with q_j=\u03c6(x_j), k_i=\u03c6(x_i), v_i=p_i, the attention value at token j computes (Kp)_j = \u2211_i \u27e8\u03c6_j, \u03c6_i\u27e9 p_i.</li> <li>Aggregator (Lemma B): an aggregator token, attending to supports, computes reductions \u2211_i r_i^2 and \u2211_i p_i (Ap)_i and broadcasts the resulting scalars back to supports.</li> <li>Token-wise affine updates (Lemma C): a per-token MLP with residual updates performs \u03b1\u2190\u03b1+\u03b3 p, r\u2190r\u2212\u03b3 Ap, p\u2190r+\u03b2 p using the broadcast \u03b3, \u03b2.</li> </ul> <p>Causal mask (S5):</p> <ul> <li>Compute phase: supports\u2194supports and aggregator\u2192support enabled; query isolated.</li> <li>Readout phase: query\u2192supports (and aggregator) enabled; supports do not read query.</li> </ul> <p>Correctness:</p> <ul> <li>Each CG step requires: two reductions (rr, pAp), one mat-vec (Kp), and three affine updates. The heads/MLP implement these operations exactly at finite precision.</li> <li>Adding \u03bb is a per-token residual add (\u03bb p) folded into Ap.</li> </ul> <p>Rate bound:</p> <ul> <li>Standard CG rate applies: \\( ||\u03b1_t\u2212\u03b1_*||_{K+\u03bbI} \u2264 2((\u221a\u03ba\u22121)/(\u221a\u03ba+1))^t ||\u03b1_*||_{K+\u03bbI} \\) with \u03ba=cond(K+\u03bbI), hence prediction error \\(|f_t(x)\u2212f_*(x)| \u2264 \u221a{k(x)^T (K+\u03bbI)^{-1} k(x)}\u00d72((\u221a\u03ba\u22121)/(\u221a\u03ba+1))^t ||\u03b1_*||_{K+\u03bbI}.\\)</li> </ul> <p>Softmax bridge:</p> <ul> <li>Replace linear attention logits with softmax logits S/\u03c4; exp(S) recovers an exponential kernel; normalization handled via aggregator or auxiliary channels.</li> </ul> <p>Notes:</p> <ul> <li>Width m\u2265p+c suffices to carry \u03c6 and CG channels; m&lt;p induces a rank-m sketch K\u0302 with spectral-tail bias (see width\u2013rank note).</li> <li>Numerical stability: float32 runtime with double-precision tests; \u03b5-machine noise leads to negligible drift under bounded t.</li> </ul>"},{"location":"theory/width_rank/","title":"Width\u2013Rank Theorem \u2014 Statement and Implications","text":"<p>Setup: Let \u03a6 \u2208 R^{k\u00d7p} be support features from lower layers. A depth-d transformer block above \u03a6 has width m channels available to carry feature-aligned computations. Consider kernel ridge regression with kernel K=\u03a6\u03a6^T (or the Route-A exponential kernel \\(\\tilde K\\)).</p> <p>Theorem (Width\u2013rank sketching): If m \u2265 p + c (channels for CG state), then there exists a parameterization realizing t CG steps on KRR exactly (up to numeric precision) under the masks described in S5. If m &lt; p, any computation that linearly preserves \u03a6 across layers induces an effective rank-m sketch \\(\\hat K\\) with spectral approximation error controlled by the tail of K:</p> \\[ \\| K - \\hat K \\|_2 \\leq \\sum_{i&gt;m} \\sigma_i(K), \\quad \\text{and} \\quad \\| (K+\\lambda I)^{-1} - (\\hat K+\\lambda I)^{-1} \\|_2 \\lesssim \\frac{1}{\\lambda^2} \\sum_{i&gt;m} \\sigma_i(K), \\] <p>which yields a query prediction error bound for any x with feature vector \u03c6(x):</p> \\[ |k(x)^\\top [(K+\\lambda I)^{-1} - (\\hat K+\\lambda I)^{-1}] y| \\;\\le\\; \\|k(x)\\|_2 \\cdot \\|y\\|_2 \\cdot \\|(K+\\lambda I)^{-1} - (\\hat K+\\lambda I)^{-1}\\|_2. \\] <p>Moreover, writing the effective dimension \\( d_\\text{eff}(\\lambda) = \\mathrm{tr}(K (K+\\lambda I)^{-1}) = \\sum_i \\frac{\\sigma_i(K)}{\\sigma_i(K)+\\lambda} \\), width m controls approximation via the spectral tail beyond m: when \\(m \\ge d_\\text{eff}(\\lambda)\\), the induced error is small; when \\(m \\ll d_\\text{eff}(\\lambda)\\), prediction degrades in proportion to the tail mass \\(\\sum_{i&gt;m} \\sigma_i(K)\\).</p> <p>Implications:</p> <ul> <li>Depth\u2013accuracy: unchanged from CG rate provided the mat-vec uses K (or \\(\\tilde K\\)); with sketch \\(\\hat K\\), an additional bias arises from \\(K\\to\\hat K\\) perturbation.</li> <li>Diagnostics: report both prediction error and \\(d_\\text{eff}(\\lambda)\\); sweeping m should track the spectral tail predictions.</li> <li>Softmax Route A: identical statements hold with K replaced by \\(\\tilde K\\) and \u03c3_i replaced by the spectrum of \\(\\tilde K\\).</li> </ul> <p>Notes: Constants can be tightened with standard resolvent perturbation bounds; operator-norm statements can be strengthened to relative error bounds under eigengap assumptions.</p>"}]}